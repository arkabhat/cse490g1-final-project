{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1b8694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.10.0+cu102\n",
      "CUDA enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('Version', torch.__version__)\n",
    "print('CUDA enabled:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e503fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import pt_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0969306",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'dakshina_dataset_v1.0'\n",
    "LANG = 'ta'\n",
    "LANG_DIR = 'lexicons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bc2ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(dirpath, filename):\n",
    "    # Load a single file\n",
    "    path = os.path.join(dirpath, filename)\n",
    "    data = []\n",
    "    with open(path, \"r\") as f:\n",
    "        tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for l in tsv:\n",
    "            data.append(l)\n",
    "    return data\n",
    "\n",
    "def load_directory(dirpath, filename=None):\n",
    "    # Create a list of all data from all files in directory\n",
    "    if filename != None:\n",
    "        return load_file(dirpath, filename)\n",
    "\n",
    "    all_files = os.listdir(dirpath)\n",
    "    all_data = []\n",
    "    for f in all_files:\n",
    "        all_data.append(load_file(dirpath, f))\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Any preprocessing we may need.\n",
    "def tokenize_data(data):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    # In case we want to pad during preprocessing later on\n",
    "    max_input = 0\n",
    "    max_output = 0\n",
    "    \n",
    "    for i,j,k in data:\n",
    "        if len(j) > max_input:\n",
    "            max_input = len(j)\n",
    "        if len(i) > max_output:\n",
    "            max_output = len(i)\n",
    "        inputs.append([ord(u) for u in j])\n",
    "        outputs.append([ord(v) for v in i])\n",
    "        \n",
    "    numpy_in = np.zeros((len(inputs), max_input))\n",
    "    numpy_out = np.zeros((len(outputs), max_output))\n",
    "    \n",
    "    count = 0\n",
    "    for i in inputs:\n",
    "        while len(i) < max_input:\n",
    "            i.append(0)\n",
    "        interior = np.asarray(i)\n",
    "        numpy_in[count] = interior\n",
    "        count += 1\n",
    "    \n",
    "    count = 0\n",
    "    for j in outputs:\n",
    "        while len(j) < max_output:\n",
    "            j.append(0)\n",
    "        interior = np.asarray(j)\n",
    "        numpy_out[count] = interior\n",
    "        count += 1\n",
    "    return numpy_in, numpy_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f56181e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/iws/arkabhat/WindowsFolders/CSE490G/cse490g1-final-project\n",
      "[102.  97.  97. 114. 109.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.] [2947. 2986. 3006. 2992. 3021. 2990. 3021.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "d = load_directory(\"{}/{}/{}\".format(DATA_DIR, LANG, LANG_DIR))\n",
    "d_in, d_out = tokenize_data(d[0])\n",
    "print(d_in[0], d_out[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bfcc2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, unique_in, unique_out):\n",
    "        super(TransliterateDataset, self).__init__()\n",
    "        all_data = load_directory(data_dir)\n",
    "        in_data = \n",
    "        out_data = []\n",
    "        for d in all_data:\n",
    "            i, o = tokenize_data(d)\n",
    "            in_data.append(i)\n",
    "            out_data.append(o)\n",
    "        # Convert to tensors\n",
    "        in_tensor = torch.Tensor(in_data)\n",
    "        out_tensor = torch.Tensor(out_data)\n",
    "        self.data = torch.cat(in_data, out_data, dim=0)\n",
    "        self.in_alphabet_len = unique_in\n",
    "        self.out_alphabet_len = unique_out\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the data and label for a character sequence as described above.\n",
    "        # The data and labels should be torch long tensors.\n",
    "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
    "        # in and how far down in the chunk you are.\n",
    "        d = self.data[idx, 0], self.data[idx, 1]\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cd68178",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 6864 at dim 1 (got 68218)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-3aea9b1c3a1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mENGLISH_UNIQUE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdirect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLANG_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransliterateDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAMIL_UNIQUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mENGLISH_UNIQUE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-0d35fe78007b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, unique_in, unique_out)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Convert to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0min_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mout_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         self.data = torch.cat(in_data, out_data, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 6864 at dim 1 (got 68218)"
     ]
    }
   ],
   "source": [
    "    TAMIL_UNIQUE = 247\n",
    "    ENGLISH_UNIQUE = 26\n",
    "    direct = \"{}/{}/{}\".format(DATA_DIR, LANG, LANG_DIR)\n",
    "    ds = TransliterateDataset(direct, TAMIL_UNIQUE, ENGLISH_UNIQUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e150ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateNet(nn.Module):\n",
    "    def __init__(self, in_alph_size, out_alph_size, feature_size):\n",
    "        super(TransliterateNet, self).__init__()\n",
    "        # Encoder and Decoder RNN\n",
    "        self.encoder = nn.Embedding(in_alph_size, self.feature_size)\n",
    "        self.rnn = nn.RNN(self.feature_size, self.feature_size, 2)\n",
    "        # Decoder embedding\n",
    "        self.dec = nn.Linear(self.feature_size, out_alph_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        x = self.enc(x)\n",
    "        x, hs = self.rnn(x, hidden_state)\n",
    "        x = self.dec(x)\n",
    "        return x, hs\n",
    "\n",
    "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
    "    def inference(self, x, hidden_state=None, temperature=1):\n",
    "        x = x.view(-1, 1)\n",
    "        x, hidden_state = self.forward(x, hidden_state)\n",
    "        x = x.view(1, -1)\n",
    "        x = x / max(temperature, 1e-20)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x, hidden_state\n",
    "\n",
    "    # Predefined loss function\n",
    "    def loss(self, prediction, label, reduction='mean'):\n",
    "        loss_val = F.cross_entropy(prediction, label)\n",
    "        return loss_val\n",
    "\n",
    "    # Saves the current model\n",
    "    def save_model(self, file_path, num_to_keep=1):\n",
    "        pt_util.save(self, file_path, num_to_keep)\n",
    "\n",
    "    # Saves the best model so far\n",
    "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.save_model(file_path, num_to_keep)\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        pt_util.restore(self, file_path)\n",
    "\n",
    "    def load_last_model(self, dir_path):\n",
    "        return pt_util.restore_latest(self, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb640f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ripped from HW 1\n",
    "import time\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss(output, label)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                time.ctime(time.time()),\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test(model, device, test_loader, log_interval=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(test_loader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
    "            test_loss += test_loss_on\n",
    "            pred = output.max(1)[1]\n",
    "            correct_mask = pred.eq(label.view_as(pred))\n",
    "            num_correct = correct_mask.sum().item()\n",
    "            correct += num_correct\n",
    "            if log_interval is not None and batch_idx % log_interval == 0:\n",
    "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    time.ctime(time.time()),\n",
    "                    batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a27495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transliteration(model, input_chars):\n",
    "    transliterations = []\n",
    "    hidden = None\n",
    "    \n",
    "    for c in input_chars:\n",
    "        x, hidden = model.inference(c, hidden)\n",
    "        transliterations.append(torch.argmax(x))\n",
    "        \n",
    "    return transliterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a93ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pass\n",
    "    # Load train and test datasets into dataloaders\n",
    "    # Train for n epochs\n",
    "    # print accuracies\n",
    "    # Check generation of transliteration with random english word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
