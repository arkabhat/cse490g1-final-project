{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1b8694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.10.0+cu102\n",
      "CUDA enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('Version', torch.__version__)\n",
    "print('CUDA enabled:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e503fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "\n",
    "import pt_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0969306",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'dakshina_dataset_v1.0'\n",
    "LANG = 'ta'\n",
    "LANG_DIR = 'lexicons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2ed6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(dirpath, filename):\n",
    "    # Load a single file\n",
    "    path = os.path.join(dirpath, filename)\n",
    "    data = []\n",
    "    with open(path, \"r\") as f:\n",
    "        tsv = csv.reader(f, delimiter=\"\\t\")\n",
    "        for l in tsv:\n",
    "            data.append(l)\n",
    "    return data\n",
    "\n",
    "def load_directory(dirpath, filename=None):\n",
    "    # Create a list of all data from all files in directory\n",
    "    if filename != None:\n",
    "        return load_file(dirpath, filename)\n",
    "\n",
    "    all_files = os.listdir(dirpath)\n",
    "    all_data = []\n",
    "    for f in all_files:\n",
    "        all_data.append(load_file(dirpath, f))\n",
    "    \n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56181e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, generate lookup tables\n",
    "ENGLISH = \"ABCDEFGHIJKLMNOPQRSTUVQXYZ\"\n",
    "ENGLISH_ALPHABET = [chr(i) for i in range(97, 97 + 26)]\n",
    "TAMIL_ALPHABET = [chr(i) for i in range(2944, 3073)]\n",
    "ENGLISH_LOOKUP = {}\n",
    "TAMIL_LOOKUP = {}\n",
    "\n",
    "ENGLISH_LOOKUP[\"pad\"] = 0\n",
    "TAMIL_LOOKUP[\"pad\"] = 0\n",
    "\n",
    "count = 1\n",
    "for a in ENGLISH_ALPHABET:\n",
    "    ENGLISH_LOOKUP[a] = count\n",
    "    count += 1\n",
    "\n",
    "count = 1\n",
    "for a in TAMIL_ALPHABET:\n",
    "    TAMIL_LOOKUP[a] = count\n",
    "    count += 1\n",
    "\n",
    "ENGLISH_LOOKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "07efad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any preprocessing we may need\n",
    "\n",
    "def tokenize_data(data):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    for i,j,k in data:\n",
    "        inputs.append([ENGLISH_LOOKUP[u] for u in j])\n",
    "        outputs.append([TAMIL_LOOKUP[v] for v in i])\n",
    "        \n",
    "    return inputs, outputs\n",
    "\n",
    "def pad_dataset(data):\n",
    "    max_len = len(max(data, key=len))\n",
    "    np_dataset = np.zeros((len(data), max_len))\n",
    "    for i in range(len(np_dataset)):\n",
    "        d = data[i]\n",
    "        while len(d) < max_len:\n",
    "            d.append(0)\n",
    "        np_dataset[i] = d\n",
    "    return np_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b511bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"{}/{}/{}\".format(DATA_DIR, LANG, LANG_DIR)\n",
    "data = load_directory(direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bfcc2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        super(TransliterateDataset, self).__init__()\n",
    "        # Load data\n",
    "        all_data = load_directory(data_dir)\n",
    "        eng_data = []\n",
    "        tam_data = []\n",
    "        for d in all_data:\n",
    "            inp, outp = tokenize_data(d)\n",
    "            eng_data += inp\n",
    "            tam_data += outp\n",
    "            \n",
    "        # Pad and convert \n",
    "        self.english_data = pad_dataset(eng_data)\n",
    "        self.tamil_data = pad_dataset(tam_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the data and label for a character sequence as described above.\n",
    "        # The data and labels should be torch long tensors.\n",
    "        # You should return a single entry for the batch using the idx to decide which chunk you are \n",
    "        # in and how far down in the chunk you are.\n",
    "        \n",
    "        d = torch.Tensor(self.english_data[idx]), torch.Tensor(self.tamil_data[idx])\n",
    "        return d\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd68178",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = \"{}/{}/{}\".format(DATA_DIR, LANG, LANG_DIR)\n",
    "ds = TransliterateDataset(direct)\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e150ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransliterateNet(nn.Module):\n",
    "    def __init__(self, in_alph_size, out_alph_size, feature_size):\n",
    "        super(TransliterateNet, self).__init__()\n",
    "        # Encoder and Decoder RNN\n",
    "        self.encoder = nn.Embedding(in_alph_size, self.feature_size)\n",
    "        self.rnn = nn.RNN(self.feature_size, self.feature_size, 2)\n",
    "        # Decoder embedding\n",
    "        self.dec = nn.Linear(self.feature_size, out_alph_size)\n",
    "        \n",
    "    def forward(self, x, hidden_state=None):\n",
    "        x = self.enc(x)\n",
    "        x, hs = self.rnn(x, hidden_state)\n",
    "        x = self.dec(x)\n",
    "        return x, hs\n",
    "\n",
    "    # This defines the function that gives a probability distribution and implements the temperature computation.\n",
    "    def inference(self, x, hidden_state=None, temperature=1):\n",
    "        x = x.view(-1, 1)\n",
    "        x, hidden_state = self.forward(x, hidden_state)\n",
    "        x = x.view(1, -1)\n",
    "        x = x / max(temperature, 1e-20)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x, hidden_state\n",
    "\n",
    "    # Predefined loss function\n",
    "    def loss(self, prediction, label, reduction='mean'):\n",
    "        loss_val = F.cross_entropy(prediction, label)\n",
    "        return loss_val\n",
    "\n",
    "    # Saves the current model\n",
    "    def save_model(self, file_path, num_to_keep=1):\n",
    "        pt_util.save(self, file_path, num_to_keep)\n",
    "\n",
    "    # Saves the best model so far\n",
    "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.save_model(file_path, num_to_keep)\n",
    "            self.best_accuracy = accuracy\n",
    "\n",
    "    def load_model(self, file_path):\n",
    "        pt_util.restore(self, file_path)\n",
    "\n",
    "    def load_last_model(self, dir_path):\n",
    "        return pt_util.restore_latest(self, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb640f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ripped from HW 1\n",
    "import time\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = model.loss(output, label)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                time.ctime(time.time()),\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return np.mean(losses)\n",
    "\n",
    "def test(model, device, test_loader, log_interval=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, label) in enumerate(test_loader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss_on = model.loss(output, label, reduction='sum').item()\n",
    "            test_loss += test_loss_on\n",
    "            pred = output.max(1)[1]\n",
    "            correct_mask = pred.eq(label.view_as(pred))\n",
    "            num_correct = correct_mask.sum().item()\n",
    "            correct += num_correct\n",
    "            if log_interval is not None and batch_idx % log_interval == 0:\n",
    "                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    time.ctime(time.time()),\n",
    "                    batch_idx * len(data), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader), test_loss_on))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a27495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transliteration(model, input_chars):\n",
    "    transliterations = []\n",
    "    hidden = None\n",
    "    \n",
    "    for c in input_chars:\n",
    "        x, hidden = model.inference(c, hidden)\n",
    "        transliterations.append(torch.argmax(x))\n",
    "        \n",
    "    return transliterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a93ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    pass\n",
    "    # Load train and test datasets into dataloaders\n",
    "    # Train for n epochs\n",
    "    # print accuracies\n",
    "    # Check generation of transliteration with random english word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
